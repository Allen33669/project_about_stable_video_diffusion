{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>PEFT (Parameter-Efficient Fine-Tuning) and LoRA (Low-Rank Adaptation): A brief introduction to LoRA</b></h1>\n",
        "<ul>\n",
        "  <li>This article is licensed under a <a href=https://creativecommons.org/licenses/by/4.0/>CC BY 4.0</a> License.</li>\n",
        "  <li><a href=https://github.com/Allen33669/stable_video_diffusion_project>source github</a></li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "QRj1JFeTPBFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Please forgive any errors or omissions.</b></h1>\n",
        "<h2>Content:<h2>\n",
        "<blockquote>\n",
        "<h5>\n",
        "PEFT brief introduction and Why LoRA?<br>\n",
        "LoRA position / layer<br>\n",
        "LoRA rank<br>\n",
        "LoRA learning rate<br>\n",
        "LoRA architecture<br>\n",
        "Multiple LoRA / Multi-task<br>\n",
        "LoRA composition<br>\n",
        "LoRA generalized framework<br>\n",
        "LoRA mixture of experts brief introduction<br>\n",
        "LoRA MoE<br>\n",
        "REFERENCES<br>\n",
        "</h5>\n",
        "</blockquote>"
      ],
      "metadata": {
        "id": "4squKZoSPBmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>PEFT brief introduction and Why LoRA?</h2>\n",
        "Parameter-Efficient Fine-Tuning (PEFT) technology demonstrates notable costeffectiveness during the fine-tuning process. This technique minimizes the trainable parameters and computational overhead while aspiring to near fully fine-tuned performance on downstream tasks [1].\n",
        "<blockquote>\n",
        "<h3>Source:</h3>\n",
        "<blockquote>\n",
        "Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies [2].<br>\n",
        "Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment [3].<br>\n",
        "When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method [4].<br>\n",
        "</blockquote>\n",
        "</blockquote>\n",
        "<br>\n",
        "<blockquote>\n",
        "<h3>Considerations factors:</h3>\n",
        "<blockquote>\n",
        "Model architecture: Will the model structure change?<br>\n",
        "Training: Forward propagation, backward propagation, optimization algorithms, memory usage, and etc.<br>\n",
        "Inference: Forward propagation, memory usage, and etc.<br>\n",
        "Forgetting: Can the base model parameters be preserved?<br>\n",
        "Flexibility: Convenient switching of tasks, merging of knowledge, etc.<br>\n",
        "Stable: Is the input stable?<br>\n",
        "</blockquote>\n",
        "</blockquote>\n",
        "<br>\n",
        "<blockquote>\n",
        "<h3>PEFT types:</h3>\n",
        "<blockquote>\n",
        "<h5>Selective PEFT:</h5>\n",
        "\"Methods for this category refer to either selectively finetuning a subset of the original model’s parameters while keeping the rest frozen, or introducing a minimal number of additional parameters to train, without altering the original parameters\" [1].<br>\n",
        "<blockquote>\n",
        "<h6>Advantage:</h6>\n",
        "It has little impact on the model structure.<br>\n",
        "No significant impact on inference costs: the method add few or no parameters to the base model.<br>\n",
        "<h6>Disadvantage:</h6>\n",
        "Forgetting: the parameters of the base model are affected and the forgetting effect may occur.<br>\n",
        "Memory risk: \"Some techniques within this category involve the integration of a masking matrix, which results in a spike in memory usage\" [1].<br>\n",
        "Training time cost risk: \"Some methods might lead to longer training periods due to a special selection mechanism. This could potentially offset the benefits of having fewer trainable parameters\" [1].<br>\n",
        "</blockquote>\n",
        "</blockquote>\n",
        "<br>\n",
        "<blockquote>\n",
        "<h5>Additive PEFT:</h5>\n",
        "\"The core idea behind adapters is to learn a set of parameters that can transform the output of one layer into the input of the next layer in a given task-specific way. Adapters are small parameter sets that can be inserted between the layers of FMs. They allow the network to be fine-tuned for a new task without modifying its original parameters\" [1].<br>\n",
        "<blockquote>\n",
        "<h6>Advantage:</h6>\n",
        "Keep the original parameters of the base model.<br>\n",
        "Easy to switch tasks: Can use different adapters for different tasks.<br>\n",
        "Integrate the knowledge: \"Can integrate the knowledge of various tasks without forgetting the knowledge from previous tasks\" [1].<br>\n",
        "<h6>Disadvantage:</h6>\n",
        "Inference cost: \"This category may cause an increase in inference overhead due to the additional computation required by the adapter layer\" [1].<br>\n",
        "Configuration issue: \"This category of methods may require careful initialization and training strategies, such as optimal settings of adapter dimensions and sparsity rates\" [1].<br>\n",
        "</blockquote>\n",
        "</blockquote>\n",
        "<br>\n",
        "<blockquote>\n",
        "<h5>Prompt PEFT:</h5>\n",
        "\"This category involves incorporating a carefully designed prompt into the input or the transformer’s layers, aiming to align the input distribution with the original training data and guide the model toward generating the desired output\" [1].<br>\n",
        "<blockquote>\n",
        "<h6>Advantage:</h6>\n",
        "Keep the original parameters of the base model: Maybe just adjust the embedding.<br>\n",
        "Possible no training cost: Maybe don’t need to retrain the model like hard prompt.<br>\n",
        "<h6>Disadvantage:</h6>\n",
        "Performance is sometimes poor compared to other methods.<br>\n",
        "Poor Transferability: \"Some prompts trained for specific a task cannot be directly transferred to other tasks. Because the prompt vectors for each task are optimized based on the data and features of that task, they have strong task-specific characteristics and are not easily generalized across different tasks\" [1].<br>\n",
        "Model dependency: \"This category of PEFT relies on the model’s already possessed capabilities. If the FMs have some deficiencies, it is difficult to compensate for these shortcomings through prompt tuning, and the room for performance improvement is limited\" [1].<br>\n",
        "Unstable: \"Sometimes relies on human input\" [1].<br>\n",
        "Suboptimal performance: \"Including prompt tokens in the input sequence can reduce the effective sequence length, potentially leading to suboptimal performance\" [1].<br>\n",
        "</blockquote>\n",
        "</blockquote>\n",
        "<br>\n",
        "<blockquote>\n",
        "<h5>Reparameterization PEFT:</h5>\n",
        "\"This technique reparameterizes the low-dimensional representation of the initial model parameters for training while converting the weights back for inference\" [1].<br>\n",
        "<blockquote>\n",
        "<h6>Advantage:</h6>\n",
        "Keep the original parameters of the base model.<br>\n",
        "High flexibility: \"It can be applied to almost all mainstream models and is very flexible, allowing for rapid adaptation to new tasks and domains\" [1].<br>\n",
        "Easy to switch tasks: Can use different LoRAs for different tasks.<br>\n",
        "Integrate the knowledge: Can integrate the knowledge of various tasks without forgetting the knowledge from previous tasks.<br>\n",
        "No significant impact on inference costs: Adjust model parameters directly for inference.<br>\n",
        "<h6>Disadvantage:</h6>\n",
        "Hyperparameters sensitivity: \"This type of method is sensitive to hyperparameters. Like, the rank of the inserted adaptation matrices significantly impacts the ability to adapt the model to a new task\" [1].<br>\n",
        "Limited representation: \"This category of PEFT assumes that model adaptations can be represented using low-rank matrices. In tasks where the feature space is highly complex, this assumption may limit expressiveness and lead to suboptimal performance\" [1].<br>\n",
        "</blockquote>\n",
        "</blockquote>\n",
        "<br>\n",
        "</blockquote>\n",
        "<br>"
      ],
      "metadata": {
        "id": "8rx8nUI5QExZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>LoRA position / layer</h2>\n",
        "Where to apply LoRA? Transformer Layers? First layer? Last layer? All layer?\n",
        "<blockquote>\n",
        "<h3>Source:</h3>\n",
        "<blockquote>\n",
        "Tied-Lora: Enhancing parameter efficiency of LoRA with weight tying [5].<br>\n",
        "LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation [6].<br>\n",
        "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning [7].<br>\n",
        "</blockquote>\n",
        "Refer to Table III [5], \"Interestingly, we note that when applying LoRA to a single transformer layer, the lower layers (usually layer 4 or 8) resulted in higher performance than higher layers. This suggests that there is potentially a single low-rank update that can be applied to all layers to boost performance, but it is hard to find a low-rank update for a single-layer that results in strong performance.\" [5].<br><br>\n",
        "Refer to Fig. 2 [6], \"Observations show that the squared norm of ∆Wixi for certain layers consistently remains close to zero, indicating that LoRA for these layers has almost no impact on the frozen model. Conversely, some layers show a more significant impact on the frozen model.\" [6].<br><br>\n",
        "Refer to Fig. 4 [6], \"We observe that the importance distributions differ across datasets, indicating that the importance assigned by LoRA is data-dependent.\" [6].<br><br>\n",
        "Refer to Fig. 5 [6], \"The results of LoRA for Query and Value are shown in Figure 5 and Figure 12. As the training data increases, the importance order of each layer remains consistent. For LoRA applied to the query matrices, the 10th layer has always been the most important, while the importance of layers 7, 8, and 9 maintains a consistently high level of importance. Indicating that this operation is insensitive to the size of the sampled data and exhibits robustness.\" [6].<br><br>\n",
        "Refer to Fig. 1 [7], \"We compare the performance of LoRA when fine-tuning specific modules or layers with the same number of trainable parameters. Figure 1a shows that fine-tuning feed-forward networks (FFN) achieves better performance than self-attention modules. In addition, Figure 1b demonstrates that weight matrices in top layers are more important than those in bottom layers.\" [7].<br><br>\n",
        "Refer to Fig. 3 [7], \"Figure 3 shows the resulting rank of each incremental matrix of DeBERTaV3-base fine-tuned with AdaLoRA. We find that AdaLoRA always prefers to allocating more budget to FFNs and top layers. Such behavior aligns with our empirical conclusions presented in Figure 1 that weight matrices of FFN moduels and top layers are more important for model performance. \" [7].<br><br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "The importance of LoRA is not equal: The importance of LoRA may vary according to different layers, different positions (such as Q, K, V), different tasks, etc.<br><br>\n",
        "Performance: Generally speaking, the top layer may be more influential, and increasing the training set may not have much impact.<br><br>\n",
        "</blockquote>\n"
      ],
      "metadata": {
        "id": "5ur2t5FxWN0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>LoRA rank</h2>\n",
        "How many ranks are suitable?\n",
        "<blockquote>\n",
        "<h3>Source:</h3>\n",
        "<blockquote>\n",
        "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning [7].<br>\n",
        "Flora: Low-Rank Adapters Are Secretly Gradient Compressors [8].<br>\n",
        "A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA [9].<br>\n",
        "LoRA Training in the NTK Regime has No Spurious Local Minima [10].<br>\n",
        "DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation [11].<br>\n",
        "</blockquote>\n",
        "Refer to Fig. 2 [7], \"Figure 2 illustrates experimental results of fine-tuning DeBERTaV3-base under different budget levels.\" [7]. As the parameter ratio of LoRA increases, the performance is generally positively correlated.<br><br>\n",
        "Refer to Table I [8], Table II [8], and Table IV [8], As the rank increases, the memory used increases, and the performance also increases, but not in equal proportion. Sometimes the performance improvement is not as good as the proportion of memory increase, and sometimes the opposite is true.<br><br>\n",
        "Refer to Fig. 2 [9], \"The study (Ding et al., 2022) asserts that fine-tuning on an increased number of parameters tends to perform better, with full-model fine-tuning consistently outperforming parameter efficient methods. Therefore we have reason to conjecture that training with larger ranks should outperform training with smaller ranks. Indeed, as illustrated in figure 2, we find that rsLoRA unlocks this performance increase for larger ranks, while LoRA’s overly aggressive scaling factor collapses and slows learning with larger ranks such that there is little to no performance difference when compared to low ranks.\" [9].<br><br>\n",
        "Refer to Fig. 3 [9], \"Validating our predictions, we illustrate in figure 3 that LoRA has collapsing gradients with higher ranks, whereas rsLoRA maintains the same gradient norm for each rank at the onset of training, while the norms remain approximately within the same order of magnitude throughout the training process.\" [9].<br><br>\n",
        "Refer to Fig. 2 [10] and Fig. 3 [10], The training loss of Lora will gradually converge to full fine tuning as the number of epochs increases, which means that more epochs are needed to train using Lora. In addition, the convergence speed is faster as the rank increases, but this effect is not obvious when the rank exceeds a certain level.<br><br>\n",
        "Refer to Table II [11], As Lora's rank increases, performance improves, but the improvement is getting smaller and smaller.<br><br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "Performance: Generally speaking, as Lora's rank increases, its performance will also increase, but the increase will become smaller and smaller or not increase in the same proportion. Sometimes, the performance will decrease as the rank increases. As the rank of Lora increases, the influence of a single parameter in Lora will decrease.<br><br>\n",
        "Training cost: Although using Lora can reduce the parameters of training, it requires more epochs. As the rank of Lora increases, the training loss decreases faster, but the effect is not obvious when the rank exceeds a threshold.<br><br>\n",
        "Memory usage: As lora ranks increase, memory usage also increases, mainly because the optimizer needs to use more memory.<br><br>\n",
        "</blockquote>"
      ],
      "metadata": {
        "id": "bLk3sdHtXwvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>LoRA learning rate</h2>\n",
        "<blockquote>\n",
        "<h3>Source:</h3>\n",
        "<blockquote>\n",
        "LoRA+: Efficient Low Rank Adaptation of Large Models [12].<br>\n",
        "</blockquote>\n",
        "Refer to Fig. 2 [12], \"We observe that both the best train and test losses are consistently achieved by a combination of learning rates where ηb ≫ ηa, which validates our analysis in the previous section. Notice also that optimal learning rates (ηA, ηB) are generally close to the edge of stability, a well-known behaviour in training dynamics of deep networks (Cohen et al., 2021).\" [12].<br><br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "Performance: Generally speaking, the learning rate of project-up matrix B is set higher than that of project-down matrix A, and the training loss is smaller for the same learning rate.<br><br>\n",
        "</blockquote>"
      ],
      "metadata": {
        "id": "oK9_d4N7ZFZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>LoRA architecture</h2>\n",
        "The structure of LoRA not only has the classic project-up matrix B and project-down matrix A, but also tensor train based low-rank adaptation, singular value decomposition based adaptation, and etc.\n",
        "<blockquote>\n",
        "<h3>Source:</h3>\n",
        "<blockquote>\n",
        "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning [7].<br>\n",
        "Tensor Train Low-rank Approximation (TT-LoRA): Democratizing AI with Accelerated LLMs [13].<br>\n",
        "</blockquote>\n",
        "AdaLoRA: AdaLoRA simulates singular value decomposition (SVD), and then truncates parameters by importance scoring.<br><br>\n",
        "TT-LoRA: TT-LoRA uses tensor train decomposition. The matrix is ​​decomposed into many small tensor cores. The head and tail matrices are 2D, and all the intermediate matrices are 3D.<br><br>\n",
        "Different LoRA structures may result in fewer parameters, but the computational cost required for training varies according to different methods. At the same time, the phenomena observed in the classic LoRA structure may not necessarily apply to other LoRA structures.<br><br>\n",
        "</blockquote>"
      ],
      "metadata": {
        "id": "XnD1W8BdZZwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Multiple LoRA / Multi-task</h2>\n",
        "\"While the original LoRA method significantly enhanced the efficiency of fine-tuning and demonstrated performance comparable to full fine-tuning, it had limitations in flexibility, generalization, and handling multiple diverse tasks simultaneously. To address these limitations, researchers have developed advanced LoRA architectures to further improve performance, parameter efficiency, and generalization ability.\" [14].<br>\n",
        "<br>\n",
        "LoRA is no longer just reducing training parameters. Can we even achieve a comprehensive performance improvement over the base model by using multiple LoRAs? Can it accomplish tasks that the base model cannot do well?<br>\n",
        "<blockquote>\n",
        "<h3>Considerations factors:</h3>\n",
        "<blockquote>\n",
        "Performance on more complex tasks: Use multiple LoRAs to complete tasks that the base model cannot do well.<br>\n",
        "<br>\n",
        "Generalization / Regularization improvement: Using LoRA can enhance the performance of a specific task, but it may also cause a forgetting effect, making the performance of other tasks worse. Is it possible to use multiple LoRA to enhance the performance of different tasks at the same time?\n",
        "<br>\n",
        "</blockquote>\n",
        "</blockquote>"
      ],
      "metadata": {
        "id": "3Uk6FcHXc41l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>LoRA composition</h2>\n",
        "\"One major innovation in advanced LoRA architectures is the dynamic composition of multiple LoRA modules to enhance adaptability and generalization across diverse tasks.\" [14].<br>\n",
        "<blockquote>\n",
        "<h3>Source:</h3>\n",
        "<blockquote>\n",
        "Mixture of LoRA Experts [15].<br>\n",
        "Multi-LoRA Composition for Image Generation [16].<br>\n",
        "LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild [17].<br>\n",
        "</blockquote>\n",
        "Refer to Table I [15], Table II [15], and Table IV [15], By combining different loras with appropriate weights, the performance of various tasks can be improved.<br><br>\n",
        "Refer to Table VI [15], As the number of LoRAs used increases, performance will also improve, but performance may not necessarily improve after LoRAs exceed a certain number.<br><br>\n",
        "Refer to Fig. 3 and Fig. 4 [16], \"LoRA-S shows superior performance in composition quality, whereas LoRA-C excels in image quality.\" [16]. There are many indicators or requirements in the benchmark (such as composition quality, image quality, and etc.). If we regard switch LoRA as a special case with a specific lora weight of one, then different LoRA weight combinations are suitable for different needs, and perhaps no specific set of weights is an overall winner.<br><br>\n",
        "Refer to Fig. 3 [16], \"The task of compositional image generation remains highly challenging, especially as the number of elements to be composed increases. According to GPT-4V’s scoring, the average score for composing 2 LoRAs is above 8.5, but it sharply declines to around 6 for compositions involving 5 LoRAs.\" [16].<br><br>\n",
        "Refer to Table I [17], \"Additionally, the three distinct strategies for LoRA composition are: (1) Selection, which involves choosing the highest-ranked (top-1) retrieved LoRA and applying it in a singular LoRA manner, and can be viewed as a variant for Mixture and Fusion methods; (2) Mixture, which averaging the outputs of each submodule from the top-k retrieved LoRAs; and (3) Fusion, a method that averages the parameters of the top-k retrieved LoRAs.\" [17]. This seems to indicate that it is better to use a specific LoRA for each specific task.<br><br>\n",
        "<br>\n",
        "<br>\n",
        "Composition methods: Fusion parameters or fusion output will have performance differences.<br><br>\n",
        "No overall winner: There is no one set of weights that works for all scenarios.<br><br>\n",
        "Complex task: Multiple LoRAs can handle complex tasks, but the performance deteriorates as the complexity of the task increases.<br><br>\n",
        "Generalization / Regularization improvement: Using multiple LoRA can increase generalization and exceed the performance of the base model. The better performance seems to be to match a specific LoRA for each specific task.<br><br>\n",
        "</blockquote>"
      ],
      "metadata": {
        "id": "VvgWHgvJdzhi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>LoRA generalized framework</h2>\n",
        "\"Another advancement involves extending the LoRA architecture itself to capture both task-specific and general features more effectively.\" [14].<br>\n",
        "<blockquote>\n",
        "<h3>Source:</h3>\n",
        "<blockquote>\n",
        "Hydra: Multi-head Low-rank Adaptation for Parameter Efficient Fine-tuning [18].<br>\n",
        "</blockquote>\n",
        "Refer to Fig. 4 [18], \"It indicates that the sequential branch learns features that are similar to well-generalized pre-trained ones. On the other hand, the parallel branch learns the unique features that were not acquired during pre-training.\" [18].<br><br>\n",
        "<br><br>\n",
        "Different locations learn different features: \"It indicates that the sequential branch learns features that are similar to well-generalized pre-trained ones. On the other hand, the parallel branch learns the unique features that were not acquired during pre-training.\" [18].<br><br>\n",
        "</blockquote>"
      ],
      "metadata": {
        "id": "BQ3FkQ-gdzeZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>LoRA mixture of experts brief introduction</h2>\n",
        "\"Another important branch in the development of advanced LoRA architectures is the combination of LoRA with Mixture of Experts (MoE). MoE is a neural network architecture where multiple “expert” sub-networks specialize in different input patterns [82]. A gating mechanism routes inputs to the most appropriate experts, allowing the model to handle a wide range of tasks efficiently [83].\" [14].<br><br>\n",
        "LoRA MoE breaks away from the original purpose of LoRA, Parameter-Efficient Fine-Tuning. On the basis of Parameter-Efficient Fine-Tuning, it pursues a more customized design to comprehensively improve performance for various tasks, but also increases the complexity and cost of the system, training, and inference.<br><br>\n",
        "<blockquote>\n",
        "<h3>Source:</h3>\n",
        "<blockquote>\n",
        "A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and Applications [19].<br>\n",
        "Mixture of Experts (MoE): A Big Data Perspective [20].<br>\n",
        "</blockquote>\n",
        "<h3>MoE factors:</h3>\n",
        "<blockquote>\n",
        "Gating function: \"The gating function serves as the mathematical implementation of a router, determining how input data is allocated to designated experts.\" [19].<br><br>\n",
        "Expert networks: \"In principle, each expert network in MoE can function as an independent network model, similar to a single network model (e.g., [29], [30]). However, in practice, to ensure efficiency and scalability, expert networks are often integrated into a single network model, with specific layers replaced by MoE layers [16], [15], [21]. Notably, MoE layers can be introduced at specific levels of existing neural network models without altering the overall structure, demonstrating high flexibility and broad applicability.\" [19].<br><br>\n",
        "Routing strategy: \"A critical consideration in the design of MoE is determining the level at which experts should specialize in knowledge\" [19].<br><br>\n",
        "Training strategies: \"Due to the sparse activations inherent in MoE-based architectures, directly applying the functions and procedures used for training dense models to MoE-based sparse models is not straightforward. To improve training effectiveness, achieve faster convergence, and enhance performance on target tasks, numerous studies like [15], [17], [16], [18], [46] have explored the methods for training these sparse models.\" [19].<br><br>\n",
        "System design: \"Its nature of inherent non-uniform workload distribution and dynamic expert selection imposes strict requirements on the system’s resource allocation, communication efficiency, and overall performance optimization. This will introduce new system challenges such as increased synchronization overhead, hampering the efficiency and stability of model execution.\" [19].<br><br>\n",
        "</blockquote>\n",
        "<h3>Advantage:</h3>\n",
        "<blockquote>\n",
        "Enhanced modeling capability: \"By dividing a complex task into multiple sub-tasks, MoE can better capture local features and intricate relationships in the data, thereby improving overall modeling capability\" [20].<br><br>\n",
        "Improved generalization performance: \"Different expert networks can focus on different subspaces of the processed data separately, thereby enhancing the model’s generalization ability to new data.\" [20].<br><br>\n",
        "Increased computational efficiency: \"Since the expert networks can be trained and inferred in parallel, the computational efficiency of MoE is significantly higher than that of a single LLM.\" [20].<br><br>\n",
        "Enhanced interpretability: \"The structure of MoE makes the model’s decision-making process more transparent, which is beneficial for improving the model’s interpretability.\" [20].<br><br>\n",
        "High-dimensional sparse data modeling: \"By partitioning the original high-dimensional data into multiple relatively low-dimensional subspaces and having different expert networks perform specialized modeling, MoE can better capture the local structural information in the data, thereby improving the modeling accuracy\" [20].<br><br>\n",
        "Heterogeneous multisource data fusion: \"Different expert networks can focus on processing heterogeneous data from different sources, and the gating network can dynamically combine the outputs of the experts to achieve effective fusion of complex data.\" [20].<br><br>\n",
        "Continuous learning, online learning, and continuous optimization: \"MoE can quickly adapt to environmental changes and achieve continuous online learning of dynamic big data by adding, removing, or fine-tuning expert networks.\" [20].<br><br>\n",
        "</blockquote>\n",
        "<h3>Disadvantage:</h3>\n",
        "<blockquote>\n",
        "Training cost: More complex training loss functions.<br><br>\n",
        "Inference cost: Dynamically select experts based on input during the inference phase, increasing inference costs.<br><br>\n",
        "More complex system design.<br><br>\n",
        "</blockquote>\n",
        "</blockquote>"
      ],
      "metadata": {
        "id": "vdnORou6al0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>LoRA MoE</h2>\n",
        "<blockquote>\n",
        "<h3>Source:</h3>\n",
        "<blockquote>\n",
        "DynMoLE: Boosting Mixture of LoRA Experts Fine-Tuning with a Hybrid Routing Mechanism [21].<br>\n",
        "Each Rank Could be an Expert: Single-Ranked Mixture of Experts LoRA for Multi-Task Learning [22].<br>\n",
        "Retrieval-Augmented Mixture of LoRA Experts for Uploadable Machine Learning [23].<br>\n",
        "</blockquote>\n",
        "Refer to Table I [21], \"While MoLA achieves an average accuracy of 75.3%, showcasing that although Top-k routing is competitive, it struggles to dynamically adjust the number of active experts based on token uncertainty. On the other hand, DYNMOLE (Top-p) delivers a commendable performance with an average accuracy of 75.6%, but its pure Top-p routing mechanism does not fully exploit the flexibility required for dynamic token-expert assignment (Detailed results are shown in Appendix A.5).\" [21]. Using MoE to respond to different inputs can improve performance. Different MoE routing strategies will also lead to different performance. Dynamically selecting a routing strategy based on input characteristics can improve performance.<br><br>\n",
        "Refer to Fig. 4 [21], \"We examined the impact of various p values, ranging from 0.6 to 0.95, and found that a Top-p value of 0.75 resulted in the highest accuracy (Figure 4(d)). These results highlight the importance of selecting an optimal Top-p value to balance expert specialization and generalization, \" [21]. \"We find that increasing k to 2 provided the necessary parameter activation, resulting in the best performance improvements (Figure 4(e)), maximally avoiding overfitting issues.\" [21].<br><br>\n",
        "Refer to Table I [22], \"(1) The performance of Vanilla LoRA improves gradually as the rank increases.\" [22]. \"(4) Notably, SMoRA, which activates only 8 out of 64 ranks in LoRA, outperforms the full Vanilla LoRA model (with all 64 ranks activated)\" [22]. Increasing rank can increase performance. Each rank of LoRA has different effects on different inputs. Even if only a few ranks of LoRA are used, the performance is better than using all ranks.<br><br>\n",
        "Refer to Table II [22], \" (3) In contrast, SMoRA excels by leveraging fine-grained parameter decoupling, which enables flexible combinations of parameters and knowledge. This results in superior performance across all tasks\" [22]. Different ranks have different knowledge. Using appropriate ranks for different inputs can improve Generalization.<br><br>\n",
        "Refer to Fig. 5 [22], \"This demonstrates that different tasks can activate unique rank parameters, enabling parameter decoupling and reducing task interference. Moreover, similar tasks tend to share comparable routing distributions.\" [22]. \"The heatmaps reveal that tasks within the same task cluster exhibit similar routing patterns, suggesting that SMoRA facilitates improved sharing of related knowledge. \" [22]. Different ranks have different knowledge.<br><br>\n",
        "Refer to Fig. 7 [22], \" (1) The performance of LoRA improves as the number of total ranks increases; 2) For SMoRA, the best performance is achieved by activating 8 ranks, as the number of activated ranks governs the granularity of information sharing between tasks. \" [22].<br><br>\n",
        "Refer to Fig. 3 [23], \"The results demonstrate that the proposed MoLE mechanism effectively assigns a higher weight to the ideal LoRA for each input at the module level. Furthermore, the importance of higher-level layers in the routing process is evident, which corroborates the findings of [55].\" [23].<br><br>\n",
        "Refer to Fig. 4 [23], \"It is shown that task embeddings within the same domain are more similar, indicating that the LoraRetriever embeddings can serve as task embeddings to characterize the similarities between different tasks and can be applied for LoRA retrieval.\" [23]. Similar tasks apply similar LoRA.<br><br>\n",
        "<br><br>\n",
        "Customization: Selecting different LoRA combinations, routing strategies or ranks based on input characteristics can improve performance. However, excessive customization may lead to overfitting problems.<br><br>\n",
        "More is not better: Using more experts does not necessarily lead to improved performance.<br><br>\n",
        "Generalization improvement: Different ranks have different knowledge. Using appropriate ranks for different inputs can improve Generalization.<br><br>\n",
        "</blockquote>"
      ],
      "metadata": {
        "id": "C6-KWgGwcJO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>REFERENCES</h2>\n",
        "<blockquote>\n",
        "[1] Dan Zhang, Tao Feng, Lilong Xue, Yuandong Wang, Yuxiao Dong, Jie Tang, “Parameter-Efficient Fine-Tuning for Foundation Models,” arXiv, arXiv:2501.13787v1, 2025.<br>\n",
        "[2] Luping Wang, Sheng Chen, Linnan Jiang, Shu Pan, Runze Cai, Sen Yang, Fei Yang, \"Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies,\"  arXiv, arXiv:2410.19878v3, 2025.<br>\n",
        "[3] Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, Fu Lee Wang, \"Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment,\" arXiv, arXiv:2312.12148v1, 2023.<br>\n",
        "[4] Biao Zhang, Zhongtao Liu, Colin Cherry, Orhan Firat, \"When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method,\" arXiv, arXiv:2402.17193v1, 2024.<br>\n",
        "[5] Adithya Renduchintala, Tugrul Konuk, Oleksii Kuchaiev, \"Tied-Lora: Enhancing parameter efficiency of LoRA with weight tying,\" arXiv, arXiv:2311.09578v2, 2024.<br>\n",
        "[6] Hongyun Zhou, Xiangyu Lu, Wang Xu, Conghui Zhu, Tiejun Zhao, Muyun Yang, \"LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation,\" arXiv, arXiv:2402.07721v2, 2024.<br>\n",
        "[7] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Nikos Karampatziakis, Pengcheng He, Yu Cheng, Weizhu Chen, Tuo Zhao, \"AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning,\" arXiv, arXiv:2303.10512v2, 2023.<br>\n",
        "[8] Yongchang Hao, Yanshuai Cao, Lili Mou, \"Flora: Low-Rank Adapters Are Secretly Gradient Compressors,\" arXiv, arXiv:2402.03293v2, 2024.<br>\n",
        "[9] Damjan Kalajdzievski, \"A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA,\" arXiv, arXiv:2312.03732v1, 2023.<br>\n",
        "[10] Uijeong Jang, Jason D. Lee, Ernest K. Ryu, \"LoRA Training in the NTK Regime has No Spurious Local Minima,\" arXiv, arXiv:2402.11867v3, 2024.<br>  \n",
        "[11] Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, Ali Ghodsi, \"DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation,\" arXiv, arXiv:2210.07558v2, 2023.<br>  \n",
        "[12] Soufiane Hayou, Nikhil Ghosh, Bin Yu, \"LoRA+: Efficient Low Rank Adaptation of Large Models,\" arXiv, arXiv:2402.12354v2, 2024.<br>\n",
        "[13] Afia Anjum, Maksim E. Eren, Ismael Boureima, Boian Alexandrov, Manish Bhattarai, \"Tensor Train Low-rank Approximation (TT-LoRA): Democratizing AI with Accelerated LLMs,\" arXiv, arXiv:2408.01008v1, 2024.<br>\n",
        "[14] Menglin Yang, Jialin Chen, Yifei Zhang, Jiahong Liu, Jiasheng Zhang, Qiyao Ma, Harshit Verma, Qianru Zhang, Min Zhou, Irwin King, Rex Ying, \"Low-Rank Adaptation for Foundation Models: A Comprehensive Review,\" arXiv, arXiv:2501.00365v1, 2024.<br>\n",
        "[15] Xun Wu, Shaohan Huang, Furu Wei, \"Mixture of LoRA Experts,\" arXiv, arXiv:2404.13628v1, 2024.<br>\n",
        "[16] Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu, Jiawei Han, Weizhu Chen, \"Multi-LoRA Composition for Image Generation,\" arXiv, arXiv:2402.16843v2, 2024.<br>\n",
        "[17] Ziyu Zhao, Leilei Gan, Guoyin Wang, Wangchunshu Zhou, Hongxia Yang, Kun Kuang, Fei Wu, \"LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild,\" arXiv, arXiv:2402.09997v1, 2024.<br>\n",
        "[18] Sanghyeon Kim, Hyunmo Yang, Younghyun Kim, Youngjoon Hong, Eunbyung Park, \"Hydra: Multi-head Low-rank Adaptation for Parameter Efficient Fine-tuning,\" arXiv, arXiv:2309.06922v1, 2023.<br>\n",
        "[19] Siyuan Mu, Sen Lin, \"A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and Applications,\" arXiv, arXiv:2503.07137v3, 2025.<br>\n",
        "[20] Wensheng Gan, Zhenyao Ning, Zhenlian Qi, Philip S. Yu, \"Mixture of Experts (MoE): A Big Data Perspective,\" arXiv, arXiv:2501.16352v1, 2025.<br>\n",
        "[21] Dengchun Li, Naizheng Wang, Zihao Zhang, Haoyang Yin, Lei Duan, Meng Xiao, Mingjie Tang, \"DynMoLE: Boosting Mixture of LoRA Experts Fine-Tuning with a Hybrid Routing Mechanism,\" arXiv, arXiv:2504.00661v1, 2025.<br>\n",
        "[22] Ziyu Zhao, Yixiao Zhou, Didi Zhu, Tao Shen, Xuwu Wang, Jing Su, Kun Kuang, Zhongyu Wei, Fei Wu, Yu Cheng, \"Each Rank Could be an Expert: Single-Ranked Mixture of Experts LoRA for Multi-Task Learning,\" arXiv, arXiv:2501.15103v1, 2025.<br>  \n",
        "[23] Ziyu Zhao, Leilei Gan, Guoyin Wang, Yuwei Hu, Tao Shen, Hongxia Yang, Kun Kuang, Fei Wu, \"Retrieval-Augmented Mixture of LoRA Experts for Uploadable Machine Learning,\" arXiv, arXiv:2406.16989v2, 2024.<br>\n",
        "</blockquote>"
      ],
      "metadata": {
        "id": "gNWJ-JnwZ064"
      }
    }
  ]
}