{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>PEFT (Parameter-Efficient Fine-Tuning) and LoRA (Low-Rank Adaptation): A brief introduction to LoRA</b></h1>\n",
        "<ul>\n",
        "  <li>This article is licensed under a <a href=https://creativecommons.org/licenses/by/4.0/>CC BY 4.0</a> License.</li>\n",
        "  <li><a href=https://github.com/Allen33669/stable_video_diffusion_project>source github</a></li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "QRj1JFeTPBFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Please forgive any errors or omissions.</b></h1>\n",
        "<h2>Content:<h2>\n",
        "<blockquote>\n",
        "<h5>\n",
        "PEFT brief introduction and Why LoRA?<br>\n",
        "LoRA position / layer<br>\n",
        "LoRA rank<br>\n",
        "LoRA learning rate<br>\n",
        "LoRA architecture<br>\n",
        "REFERENCES<br>\n",
        "</h5>\n",
        "</blockquote>"
      ],
      "metadata": {
        "id": "4squKZoSPBmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>PEFT brief introduction and Why LoRA?</h2>\n",
        "Parameter-Efficient Fine-Tuning (PEFT) technology demonstrates notable costeffectiveness during the fine-tuning process. This technique minimizes the trainable parameters and computational overhead while aspiring to near fully fine-tuned performance on downstream tasks [1].\n",
        "<blockquote>\n",
        "<h3>Source:</h3>\n",
        "<blockquote>\n",
        "Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies [2].<br>\n",
        "Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment [3].<br>\n",
        "When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method [4].<br>\n",
        "</blockquote>\n",
        "</blockquote>\n",
        "<br>\n",
        "<blockquote>\n",
        "<h3>Considerations factors:</h3>\n",
        "<blockquote>\n",
        "Model architecture: Will the model structure change?<br>\n",
        "Training: Forward propagation, backward propagation, optimization algorithms, memory usage, and etc.<br>\n",
        "Inference: Forward propagation, memory usage, and etc.<br>\n",
        "Forgetting: Can the base model parameters be preserved?<br>\n",
        "Flexibility: Convenient switching of tasks, merging of knowledge, etc.<br>\n",
        "Stable: Is the input stable?<br>\n",
        "</blockquote>\n",
        "</blockquote>\n",
        "<br>\n",
        "<blockquote>\n",
        "<h3>PEFT types:</h3>\n",
        "<blockquote>\n",
        "<h5>Selective PEFT:</h5>\n",
        "\"Methods for this category refer to either selectively finetuning a subset of the original model’s parameters while keeping the rest frozen, or introducing a minimal number of additional parameters to train, without altering the original parameters\" [1].<br>\n",
        "<blockquote>\n",
        "<h6>Advantage:</h6>\n",
        "It has little impact on the model structure.<br>\n",
        "No significant impact on inference costs: the method add few or no parameters to the base model.<br>\n",
        "<h6>Disadvantage:</h6>\n",
        "Forgetting: the parameters of the base model are affected and the forgetting effect may occur.<br>\n",
        "Memory risk: \"Some techniques within this category involve the integration of a masking matrix, which results in a spike in memory usage\" [1].<br>\n",
        "Training time cost risk: \"Some methods might lead to longer training periods due to a special selection mechanism. This could potentially offset the benefits of having fewer trainable parameters\" [1].<br>\n",
        "</blockquote>\n",
        "</blockquote>\n",
        "<br>\n",
        "<blockquote>\n",
        "<h5>Additive PEFT:</h5>\n",
        "\"The core idea behind adapters is to learn a set of parameters that can transform the output of one layer into the input of the next layer in a given task-specific way. Adapters are small parameter sets that can be inserted between the layers of FMs. They allow the network to be fine-tuned for a new task without modifying its original parameters\" [1].<br>\n",
        "<blockquote>\n",
        "<h6>Advantage:</h6>\n",
        "Keep the original parameters of the base model.<br>\n",
        "Easy to switch tasks: Can use different adapters for different tasks.<br>\n",
        "Integrate the knowledge: \"Can integrate the knowledge of various tasks without forgetting the knowledge from previous tasks\" [1].<br>\n",
        "<h6>Disadvantage:</h6>\n",
        "Inference cost: \"This category may cause an increase in inference overhead due to the additional computation required by the adapter layer\" [1].<br>\n",
        "Configuration issue: \"This category of methods may require careful initialization and training strategies, such as optimal settings of adapter dimensions and sparsity rates\" [1].<br>\n",
        "</blockquote>\n",
        "</blockquote>\n",
        "<br>\n",
        "<blockquote>\n",
        "<h5>Prompt PEFT:</h5>\n",
        "\"This category involves incorporating a carefully designed prompt into the input or the transformer’s layers, aiming to align the input distribution with the original training data and guide the model toward generating the desired output\" [1].<br>\n",
        "<blockquote>\n",
        "<h6>Advantage:</h6>\n",
        "Keep the original parameters of the base model: Maybe just adjust the embedding.<br>\n",
        "Possible no training cost: Maybe don’t need to retrain the model like hard prompt.<br>\n",
        "<h6>Disadvantage:</h6>\n",
        "Performance is sometimes poor compared to other methods.<br>\n",
        "Poor Transferability: \"Some prompts trained for specific a task cannot be directly transferred to other tasks. Because the prompt vectors for each task are optimized based on the data and features of that task, they have strong task-specific characteristics and are not easily generalized across different tasks\" [1].<br>\n",
        "Model dependency: \"This category of PEFT relies on the model’s already possessed capabilities. If the FMs have some deficiencies, it is difficult to compensate for these shortcomings through prompt tuning, and the room for performance improvement is limited\" [1].<br>\n",
        "Unstable: \"Sometimes relies on human input\" [1].<br>\n",
        "Suboptimal performance: \"Including prompt tokens in the input sequence can reduce the effective sequence length, potentially leading to suboptimal performance\" [1].<br>\n",
        "</blockquote>\n",
        "</blockquote>\n",
        "<br>\n",
        "<blockquote>\n",
        "<h5>Reparameterization PEFT:</h5>\n",
        "\"This technique reparameterizes the low-dimensional representation of the initial model parameters for training while converting the weights back for inference\" [1].<br>\n",
        "<blockquote>\n",
        "<h6>Advantage:</h6>\n",
        "Keep the original parameters of the base model.<br>\n",
        "High flexibility: \"It can be applied to almost all mainstream models and is very flexible, allowing for rapid adaptation to new tasks and domains\" [1].<br>\n",
        "Easy to switch tasks: Can use different LoRAs for different tasks.<br>\n",
        "Integrate the knowledge: Can integrate the knowledge of various tasks without forgetting the knowledge from previous tasks.<br>\n",
        "No significant impact on inference costs: Adjust model parameters directly for inference.<br>\n",
        "<h6>Disadvantage:</h6>\n",
        "Hyperparameters sensitivity: \"This type of method is sensitive to hyperparameters. Like, the rank of the inserted adaptation matrices significantly impacts the ability to adapt the model to a new task\" [1].<br>\n",
        "Limited representation: \"This category of PEFT assumes that model adaptations can be represented using low-rank matrices. In tasks where the feature space is highly complex, this assumption may limit expressiveness and lead to suboptimal performance\" [1].<br>\n",
        "</blockquote>\n",
        "</blockquote>\n",
        "<br>\n",
        "</blockquote>\n",
        "<br>"
      ],
      "metadata": {
        "id": "8rx8nUI5QExZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>LoRA position / layer</h2>\n",
        "Where to apply LoRA? Transformer Layers? First layer? Last layer? All layer?\n",
        "<blockquote>\n",
        "<h3>Source:</h3>\n",
        "<blockquote>\n",
        "Tied-Lora: Enhancing parameter efficiency of LoRA with weight tying [5].<br>\n",
        "LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation [6].<br>\n",
        "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning [7].<br>\n",
        "</blockquote>\n",
        "Refer to Table III [5], \"Interestingly, we note that when applying LoRA to a single transformer layer, the lower layers (usually layer 4 or 8) resulted in higher performance than higher layers. This suggests that there is potentially a single low-rank update that can be applied to all layers to boost performance, but it is hard to find a low-rank update for a single-layer that results in strong performance.\" [5].<br><br>\n",
        "Refer to Fig. 2 [6], \"Observations show that the squared norm of ∆Wixi for certain layers consistently remains close to zero, indicating that LoRA for these layers has almost no impact on the frozen model. Conversely, some layers show a more significant impact on the frozen model.\" [6].<br><br>\n",
        "Refer to Fig. 4 [6], \"We observe that the importance distributions differ across datasets, indicating that the importance assigned by LoRA is data-dependent.\" [6].<br><br>\n",
        "Refer to Fig. 5 [6], \"The results of LoRA for Query and Value are shown in Figure 5 and Figure 12. As the training data increases, the importance order of each layer remains consistent. For LoRA applied to the query matrices, the 10th layer has always been the most important, while the importance of layers 7, 8, and 9 maintains a consistently high level of importance. Indicating that this operation is insensitive to the size of the sampled data and exhibits robustness.\" [6].<br><br>\n",
        "Refer to Fig. 1 [7], \"We compare the performance of LoRA when fine-tuning specific modules or layers with the same number of trainable parameters. Figure 1a shows that fine-tuning feed-forward networks (FFN) achieves better performance than self-attention modules. In addition, Figure 1b demonstrates that weight matrices in top layers are more important than those in bottom layers.\" [7].<br><br>\n",
        "Refer to Fig. 3 [7], \"Figure 3 shows the resulting rank of each incremental matrix of DeBERTaV3-base fine-tuned with AdaLoRA. We find that AdaLoRA always prefers to allocating more budget to FFNs and top layers. Such behavior aligns with our empirical conclusions presented in Figure 1 that weight matrices of FFN moduels and top layers are more important for model performance. \" [7].<br><br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "The importance of LoRA is not equal: The importance of LoRA may vary according to different layers, different positions (such as Q, K, V), different tasks, etc.<br><br>\n",
        "Performance: Generally speaking, the top layer may be more influential, and increasing the training set may not have much impact.<br><br>\n",
        "</blockquote>\n"
      ],
      "metadata": {
        "id": "5ur2t5FxWN0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>LoRA rank</h2>\n",
        "How many ranks are suitable?\n",
        "<blockquote>\n",
        "<h3>Source:</h3>\n",
        "<blockquote>\n",
        "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning [7].<br>\n",
        "Flora: Low-Rank Adapters Are Secretly Gradient Compressors [8].<br>\n",
        "A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA [9].<br>\n",
        "LoRA Training in the NTK Regime has No Spurious Local Minima [10].<br>\n",
        "DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation [11].<br>\n",
        "</blockquote>\n",
        "Refer to Fig. 2 [7], \"Figure 2 illustrates experimental results of fine-tuning DeBERTaV3-base under different budget levels.\" [7]. As the parameter ratio of LoRA increases, the performance is generally positively correlated.<br><br>\n",
        "Refer to Table I [8], Table II [8], and Table IV [8], As the rank increases, the memory used increases, and the performance also increases, but not in equal proportion. Sometimes the performance improvement is not as good as the proportion of memory increase, and sometimes the opposite is true.<br><br>\n",
        "Refer to Fig. 2 [9], \"The study (Ding et al., 2022) asserts that fine-tuning on an increased number of parameters tends to perform better, with full-model fine-tuning consistently outperforming parameter efficient methods. Therefore we have reason to conjecture that training with larger ranks should outperform training with smaller ranks. Indeed, as illustrated in figure 2, we find that rsLoRA unlocks this performance increase for larger ranks, while LoRA’s overly aggressive scaling factor collapses and slows learning with larger ranks such that there is little to no performance difference when compared to low ranks.\" [9].<br><br>\n",
        "Refer to Fig. 3 [9], \"Validating our predictions, we illustrate in figure 3 that LoRA has collapsing gradients with higher ranks, whereas rsLoRA maintains the same gradient norm for each rank at the onset of training, while the norms remain approximately within the same order of magnitude throughout the training process.\" [9].<br><br>\n",
        "Refer to Fig. 2 [10] and Fig. 3 [10], The training loss of Lora will gradually converge to full fine tuning as the number of epochs increases, which means that more epochs are needed to train using Lora. In addition, the convergence speed is faster as the rank increases, but this effect is not obvious when the rank exceeds a certain level.<br><br>\n",
        "Refer to Table II [11], As Lora's rank increases, performance improves, but the improvement is getting smaller and smaller.<br><br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "Performance: Generally speaking, as Lora's rank increases, its performance will also increase, but the increase will become smaller and smaller or not increase in the same proportion. Sometimes, the performance will decrease as the rank increases. As the rank of Lora increases, the influence of a single parameter in Lora will decrease.<br><br>\n",
        "Training cost: Although using Lora can reduce the parameters of training, it requires more epochs. As the rank of Lora increases, the training loss decreases faster, but the effect is not obvious when the rank exceeds a threshold.<br><br>\n",
        "Memory usage: As lora ranks increase, memory usage also increases, mainly because the optimizer needs to use more memory.<br><br>\n",
        "</blockquote>"
      ],
      "metadata": {
        "id": "bLk3sdHtXwvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>LoRA learning rate</h2>\n",
        "<blockquote>\n",
        "<h3>Source:</h3>\n",
        "<blockquote>\n",
        "LoRA+: Efficient Low Rank Adaptation of Large Models [12].<br>\n",
        "</blockquote>\n",
        "Refer to Fig. 2 [12], \"We observe that both the best train and test losses are consistently achieved by a combination of learning rates where ηb ≫ ηa, which validates our analysis in the previous section. Notice also that optimal learning rates (ηA, ηB) are generally close to the edge of stability, a well-known behaviour in training dynamics of deep networks (Cohen et al., 2021).\" [12].<br><br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "Performance: Generally speaking, the learning rate of project-up matrix B is set higher than that of project-down matrix A, and the training loss is smaller for the same learning rate.<br><br>\n",
        "</blockquote>"
      ],
      "metadata": {
        "id": "oK9_d4N7ZFZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>LoRA architecture</h2>\n",
        "The structure of LoRA not only has the classic project-up matrix B and project-down matrix A, but also tensor train based low-rank adaptation, singular value decomposition based adaptation, and etc.\n",
        "<blockquote>\n",
        "<h3>Source:</h3>\n",
        "<blockquote>\n",
        "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning [7].<br>\n",
        "Tensor Train Low-rank Approximation (TT-LoRA): Democratizing AI with Accelerated LLMs [13].<br>\n",
        "</blockquote>\n",
        "AdaLoRA: AdaLoRA simulates singular value decomposition (SVD), and then truncates parameters by importance scoring.<br><br>\n",
        "TT-LoRA: TT-LoRA uses tensor train decomposition. The matrix is ​​decomposed into many small tensor cores. The head and tail matrices are 2D, and all the intermediate matrices are 3D.<br><br>\n",
        "Different LoRA structures may result in fewer parameters, but the computational cost required for training varies according to different methods. At the same time, the phenomena observed in the classic LoRA structure may not necessarily apply to other LoRA structures.<br><br>\n",
        "</blockquote>"
      ],
      "metadata": {
        "id": "XnD1W8BdZZwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>REFERENCES</h2>\n",
        "<blockquote>\n",
        "[1] Dan Zhang, Tao Feng, Lilong Xue, Yuandong Wang, Yuxiao Dong, Jie Tang, “Parameter-Efficient Fine-Tuning for Foundation Models,” arXiv, arXiv:2501.13787v1, 2025.<br>\n",
        "[2] Luping Wang, Sheng Chen, Linnan Jiang, Shu Pan, Runze Cai, Sen Yang, Fei Yang, \"Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies,\"  arXiv, arXiv:2410.19878v3, 2025.<br>\n",
        "[3] Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, Fu Lee Wang, \"Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment,\" arXiv, arXiv:2312.12148v1, 2023.<br>\n",
        "[4] Biao Zhang, Zhongtao Liu, Colin Cherry, Orhan Firat, \"When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method,\" arXiv, arXiv:2402.17193v1, 2024.<br>\n",
        "[5] Adithya Renduchintala, Tugrul Konuk, Oleksii Kuchaiev, \"Tied-Lora: Enhancing parameter efficiency of LoRA with weight tying,\" arXiv, arXiv:2311.09578v2, 2024.<br>\n",
        "[6] Hongyun Zhou, Xiangyu Lu, Wang Xu, Conghui Zhu, Tiejun Zhao, Muyun Yang, \"LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation,\" arXiv, arXiv:2402.07721v2, 2024.<br>\n",
        "[7] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Nikos Karampatziakis, Pengcheng He, Yu Cheng, Weizhu Chen, Tuo Zhao, \"AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning,\" arXiv, arXiv:2303.10512v2, 2023.<br>\n",
        "[8] Yongchang Hao, Yanshuai Cao, Lili Mou, \"Flora: Low-Rank Adapters Are Secretly Gradient Compressors,\" arXiv, arXiv:2402.03293v2, 2024.<br>\n",
        "[9] Damjan Kalajdzievski, \"A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA,\" arXiv, arXiv:2312.03732v1, 2023.<br>\n",
        "[10] Uijeong Jang, Jason D. Lee, Ernest K. Ryu, \"LoRA Training in the NTK Regime has No Spurious Local Minima,\" arXiv, arXiv:2402.11867v3, 2024.<br>  \n",
        "[11] Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, Ali Ghodsi, \"DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation,\" arXiv, arXiv:2210.07558v2, 2023.<br>  \n",
        "[12] Soufiane Hayou, Nikhil Ghosh, Bin Yu, \"LoRA+: Efficient Low Rank Adaptation of Large Models,\" arXiv, arXiv:2402.12354v2, 2024.<br>\n",
        "[13] Afia Anjum, Maksim E. Eren, Ismael Boureima, Boian Alexandrov, Manish Bhattarai, \"Tensor Train Low-rank Approximation (TT-LoRA): Democratizing AI with Accelerated LLMs,\" arXiv, arXiv:2408.01008v1, 2024.<br>\n",
        "</blockquote>"
      ],
      "metadata": {
        "id": "gNWJ-JnwZ064"
      }
    }
  ]
}